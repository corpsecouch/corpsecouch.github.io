<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Designing LLM interfaces: a new paradigm | Jason Bejot</title>
    <meta name="description" content="The personal website for Jason Bejot">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/assets/style.W3g-fF26.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.ItMU960Z.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.CjgJF0HT.js">
    <link rel="modulepreload" href="/assets/chunks/framework.D6M9R-ut.js">
    <link rel="modulepreload" href="/assets/articles_designing-llm-interfaces-a-new-paradigm_index.md.CsjxA3_x.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="canonical" href="https://jasonbejot.com/articles/designing-llm-interfaces-a-new-paradigm/">
    <link rel="apple-touch-icon-precomposed" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon-precomposed" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon-precomposed" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon-precomposed" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon-precomposed" sizes="167x167" href="/apple-touch-icon-167x167.png">
    <link rel="apple-touch-icon-precomposed" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="128x128" href="/favicon-128x128.png">
    <link rel="icon" type="image/png" sizes="196x196" href="/favicon-196x196.png">
  </head>
  <body>
    <div id="app"><!--[--><header class="_header_1uzdg_2"><div class="container"><span class="_name_1uzdg_23"><a href="/" alt="Home">Jason</a></span><a href="/portfolio/" alt="Portfolio">Portfolio</a><a href="/articles/" alt="Articles">Articles</a><a href="/awards/" alt="Awards">Awards</a><a href="/press/" alt="Press">Press</a><a href="/#contact" alt="Contact">Contact</a></div></header><main><div class="widthConstrained"><section class="_hero_1y04i_3"><h1><!--[-->Designing LLM interfaces: a new paradigm<!--]--></h1><p class=""><!--[--><!--]--></p><p class=""><!--[-->November 8, 2024<!--]--></p></section><section class="_content_1y04i_16"><div style="position:relative;"><div><p><img src="/assets/hero.BELregDs.jpg" alt=""></p><p>Designing an interface for an LLM is deceptively complex. You might be thinking: “Isn’t it just a boring, old chat UI?”. There are shared aspects, sure, but also it’s something completely different. It might even cause you to reframe how you approach design.</p><h3 id="what-is-an-interface" tabindex="-1">What is an interface? <a class="header-anchor" href="#what-is-an-interface" aria-label="Permalink to &quot;What is an interface?&quot;">​</a></h3><p>An interface is essentially a thing we humans use to interact with a computer. The interface is the layer between the person and a pile of circuits. Those circuits have a known set of features that the interface enables a person to use.</p><p>In this definition, there’s only one unknown: the user. We don’t know who’s using the interface or what they’re going to do with it. With research, we’d have a good idea but we ultimately don’t know. Product and UX design have become very good at designing in this paradigm, when the only unknown is the user. But what happens when we add an LLM to the equation?</p><h3 id="llms-change-the-design-paradigm" tabindex="-1">LLMs change the design paradigm <a class="header-anchor" href="#llms-change-the-design-paradigm" aria-label="Permalink to &quot;LLMs change the design paradigm&quot;">​</a></h3><p>By this point, I’m assuming most of us understand that LLMs, and generative AI in general, are non-deterministic. Just like a human user, you don’t know what they’re going to do. When you design a product with an LLM in the mix, you’re now designing for two unknowns. The design paradigm changes and the primary purpose of your UI is to enable communication between the two unknowns — the user and the LLM.</p><p>This new paradigm is one of the challenges product teams face when building for LLMs, whether you’re working at Anthropic designing the interface for Claude or integrating with ChatGPT to build a conversational AI agent for your company.</p><h3 id="more-than-a-chatbot-ui" tabindex="-1">More than a chatbot UI <a class="header-anchor" href="#more-than-a-chatbot-ui" aria-label="Permalink to &quot;More than a chatbot UI&quot;">​</a></h3><p>So how is this different from a chatbot UI? The primary difference is determinism. Chatbots are deterministic, LLMs are not. While chatbots use AI and ML for their NLP and NLU, their behaviors and responses are static and pre-designed — there’s little variability in what it will say and do. You know exactly when it will need to respond with text, an image, or a button. LLMs, however, are non-deterministic; you don’t know how it will behave or respond. Your goal is to predict what it will need.</p><h2 id="approaching-the-problem" tabindex="-1">Approaching the problem <a class="header-anchor" href="#approaching-the-problem" aria-label="Permalink to &quot;Approaching the problem&quot;">​</a></h2><p>So, how do you design for this new paradigm? My approach is simple: treat your AI as another type of user.</p><h3 id="your-llm-is-a-user" tabindex="-1">Your LLM is a user <a class="header-anchor" href="#your-llm-is-a-user" aria-label="Permalink to &quot;Your LLM is a user&quot;">​</a></h3><p>There are a lot of similarities of the unknownness between a human user and an LLM when it comes to product design. Look at your product from your AIs perspective. Then run a design process focused on it just like you would a human user. Work to understand its needs, goals, contexts… all of it. Once you understand it as a user you can design the interface it needs to communicate with the human user.</p><p>In practice, you’re running two design efforts; one for your human users and one for your LLM.</p><h3 id="designing-for-non-determinism" tabindex="-1">Designing for non-determinism <a class="header-anchor" href="#designing-for-non-determinism" aria-label="Permalink to &quot;Designing for non-determinism&quot;">​</a></h3><p>Regardless of what a user asks an LLM to do, the LLM ultimately determines what it will respond with and how it formats that response. That’s the nature of a nondeterministic system; no matter how predictable an LLM ability is, there is non-zero a chance of it responding with something unexpected. This means the interface needs to be loosely coupled and able to handle whatever your LLM with throw at it.</p><p>By treating your AI is a user, you’re defining a set of tools it can use to communicate with your human users through the interface. When your AI responds with something unexpected, unknown or emergent, your interface will be able to gracefully and dynamically handle it.</p><h3 id="why-loosely-coupled" tabindex="-1">Why loosely coupled? <a class="header-anchor" href="#why-loosely-coupled" aria-label="Permalink to &quot;Why loosely coupled?&quot;">​</a></h3><p>It’s no big secret that LLMs consist of well known and little known features. There’s also ongoing debates about unknown features, so called emergent abilities. An emergent LLM ability has been defined as “not present in smaller models but is present in larger models” (Wei, et al 2022). Essentially, they’re things the model learned to do as it scaled and not something it was intentionally trained to do.</p><p>Like I said, there’s an ongoing debate whether LLMs actually show emergent abilities or whether it’s just an artifact of how models are tested and measured:</p><ul><li><a href="https://www.wired.com/story/how-quickly-do-large-language-models-learn-unexpected-skills/" target="_blank" rel="noreferrer">https://www.wired.com/story/how-quickly-do-large-language-models-learn-unexpected-skills/</a></li><li><a href="https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/" target="_blank" rel="noreferrer">https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/</a></li><li><a href="https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/" target="_blank" rel="noreferrer">https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/</a></li></ul><p>No matter where you land on the debate, the point is these systems are so big and complex that you can’t know it all even if you’re one of the people training it. New behaviors and features you didn’t know about become surprises and feel emergent. To complicate things further, it’s a moving target; each new model has new features and behaves in different ways.</p><h2 id="context-is-critical" tabindex="-1">Context is critical <a class="header-anchor" href="#context-is-critical" aria-label="Permalink to &quot;Context is critical&quot;">​</a></h2><p>The needs of your human users and your LLM aren’t mutually exclusive. They will influence each other. Arguably, the most important aspect is context; understanding where your human users are interacting with your AI. This will dictate how you design both the interface and the interactions.</p><h3 id="modality-dictates-everything" tabindex="-1">Modality dictates everything <a class="header-anchor" href="#modality-dictates-everything" aria-label="Permalink to &quot;Modality dictates everything&quot;">​</a></h3><p>Will your human users only be using this on desktop-sized web? Or maybe it’s also through SMS? Can they can do it through WhatsApp, Slack, or Microsoft Teams?</p><p>Each modality has a different set of capabilities and restrictions. SMS is probably the most restrictive. What happens if your AI responds to the user with a code block over SMS? Or a 1,000 word poem? Or 20 images of a home you want to tour? These all sound like terrible things to get over SMS.</p><p>Your AI needs to know the users modality and what kind of responses are available for that modality. This way it can tailor its behavior to the current modality and remain loosely coupled. For example, instead of sending that code block over SMS it could respond with a download link or send it in an email; features outside the bounds of an interface.</p><p><img src="/assets/one.BSHar0b5.png" alt=""></p><h2 id="basic-llm-interface-features" tabindex="-1">Basic LLM interface features <a class="header-anchor" href="#basic-llm-interface-features" aria-label="Permalink to &quot;Basic LLM interface features&quot;">​</a></h2><p>Essentially, every LLM-based/conversational experience has the same basic needs. I’ve listed some of them out here to help kickstart your efforts, assuming you’re building a conversational interface. This isn’t an exhaustive list, but definitely a good place to start so you’re not reinventing the wheel:</p><ul><li>Plain text</li><li>Rich text</li><li>Pre-formatted content</li><li>Emojis</li><li>Code blocks</li><li>Images</li><li>Buttons</li></ul><h3 id="plain-text" tabindex="-1">Plain Text <a class="header-anchor" href="#plain-text" aria-label="Permalink to &quot;Plain Text&quot;">​</a></h3><p>The most foundational are blocks of plain text. The interface should be able to take plain text from the AI, format it and display it to the human user without any issues.</p><h3 id="rich-text" tabindex="-1">Rich Text <a class="header-anchor" href="#rich-text" aria-label="Permalink to &quot;Rich Text&quot;">​</a></h3><p>Next would be formatted text, or rich text. Thing like lists and titles, bold and italics, and links. The interface should be able to take a response with rich text from the AI, format it to fit the design system and display it to the human user without any issues.</p><p><img src="/assets/two.Z8t5-gWb.png" alt=""></p><h3 id="pre-formatted-content" tabindex="-1">Pre-formatted Content <a class="header-anchor" href="#pre-formatted-content" aria-label="Permalink to &quot;Pre-formatted Content&quot;">​</a></h3><p>There are likely scenarios where the content your LLM serves up is pre-designed and deterministic (e.g., not nondeterministic). This is likely something that lives in the interface and simply gets triggered by the LLM. Regardless, you’ll need to account for pre-designed, deterministic content.</p><p><img src="/assets/three.zttlZi_4.png" alt=""></p><h3 id="emojis" tabindex="-1">Emojis <a class="header-anchor" href="#emojis" aria-label="Permalink to &quot;Emojis&quot;">​</a></h3><p>Emojis and emoticons seem like table stakes, but they do warrant their own considerations. Your interface needs to be able to display emojis that your AI responds with, otherwise your human users are liable to see those empty square boxes sprinkled throughout their conversation.</p><p><img src="/assets/four.Db8kabby.png" alt=""></p><h3 id="code" tabindex="-1">Code <a class="header-anchor" href="#code" aria-label="Permalink to &quot;Code&quot;">​</a></h3><p>LLMs spitting out code is one of their big value props. It’s all just text but your interface should handle blocks of code coming from your AI differently so that it’s formatted in a way that easy to use and obviously different from the other types of text. One of the tricks is reliably knowing the start and end of a code block in a response.</p><p><img src="/assets/five.YqSovO5L.png" alt=""></p><h3 id="images" tabindex="-1">Images <a class="header-anchor" href="#images" aria-label="Permalink to &quot;Images&quot;">​</a></h3><p>Next up the spicy ladder of complexity is images. An image is just an encoded string of text. If the interface can’t handle images from your AI, then your human users might just see a massive block of gibberish text.</p><p><img src="/assets/six.CDIflA8-.jpg" alt=""></p><p>Have you thought about what if your AI responds with multiple images? Imagine talking with a real estate AI and it only sends you one image of a house a time. Not a great experience. Your interface should be able to handle responses with multiple images.</p><h3 id="buttons" tabindex="-1">Buttons <a class="header-anchor" href="#buttons" aria-label="Permalink to &quot;Buttons&quot;">​</a></h3><p>Buttons are important for conversational experiences and your interface should be able to handle them. They’re everywhere in the old NLU-based chatbot world in large part because they’re really effective when used judiciously. Besides, you don’t know if or when your AI will respond with a button so your interface should be prepared to accommodate them.</p><p>If that hasn’t convinced you, then keep in mind there are certain situations where a button is either legally required or needed to limit liability, for instance, accepting Terms &amp; Conditions or a Privacy Policy.</p><p><img src="/assets/seven.CTvdFcv_.png" alt=""></p><h2 id="more-complex-features" tabindex="-1">More complex features <a class="header-anchor" href="#more-complex-features" aria-label="Permalink to &quot;More complex features&quot;">​</a></h2><p>Depending on how complex you want to go, what your human users are doing, and how specialized your making your AI; there’s really no end to the types of features you’d need to make available in your interface. Videos, charts &amp; graphs, calendars, spreadsheets, screenplays, essays, resumes, maps, input fields… the list goes on.</p><h3 id="input-fields" tabindex="-1">Input fields <a class="header-anchor" href="#input-fields" aria-label="Permalink to &quot;Input fields&quot;">​</a></h3><p>This might seem weird in a conversational or chat experience, but using input fields to collect specific pieces of data can be really effective.</p><p><img src="/assets/eight.DAWriR2c.png" alt=""></p><h3 id="side-panels" tabindex="-1">Side panels <a class="header-anchor" href="#side-panels" aria-label="Permalink to &quot;Side panels&quot;">​</a></h3><p>Not everything in a conversational experience has to be presented linearly. ChatGPT puts everything in one giant chat bubble no matter how long it is. While that works for shorter responses, longer responses quickly become overwhelming. Something Claude does is put longer, contained responses in a side panel creating a nice separation between the thread of the conversation vs. the artifacts it’s producing.</p><p><img src="/assets/nine.DTfyGQM5.png" alt=""></p><h2 id="things-not-covered" tabindex="-1">Things not covered <a class="header-anchor" href="#things-not-covered" aria-label="Permalink to &quot;Things not covered&quot;">​</a></h2><p>There are lots of other features that you’ll need to implement in your interface to run a healthy, mature LLM-powered product. Things like response rating (thumbs up / thumbs down), post chat survey, timestamps, avatars, transcripts, metrics, etc. They’re critical for a well-rounded product. However, they’re not required for an interface to enable conversational communication between an AI and a human user.</p><h2 id="into-the-future" tabindex="-1">Into the future <a class="header-anchor" href="#into-the-future" aria-label="Permalink to &quot;Into the future&quot;">​</a></h2><p>This whole field is changing at a break-neck pace. As generative AI, especially LLMs, evolve the interfaces they and we need to interact with each other need to evolve at a similar pace. One thing I think this challenges is the long product development processes teams rely on. Designing for an LLM as a user provides this great hidden benefit; a way of future-proofing. It can help bridge the gap between the evolution of LLM models’ abilities and product team’s release cycles.</p></div></div></section><section class="_author_1y04i_16"><h2>About the author</h2><p>Jason Bejot is a leader of product design and strategy specializing in conversational AI. He’s a UX designer with a computer science degree with 19 years of experience having worked at Disney, Amazon, and Rocket Companies. Jason has received three patents in AR and VR and has received 15 awards for his engineering and design work.</p></section></div></main><footer class="_footer_c39hw_2 divider"> Built from scratch with care. <span class="_copyright_c39hw_10">© Jason Bejot</span></footer><!--]--></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"404.md\":\"BcGwQPAH\",\"articles_a-pro-resume-tip-you-probably-havent-heard-of_index.md\":\"BFsI4Ap3\",\"articles_designing-for-the-80-20_index.md\":\"l35ErgOQ\",\"articles_designing-llm-interfaces-a-new-paradigm_index.md\":\"CsjxA3_x\",\"articles_how-to-workshop-a-user-journey_index.md\":\"BheP9ibF\",\"articles_index.md\":\"oBezQx4R\",\"articles_the-mixed-reality-prototyping-tool-designers-need_index.md\":\"C2Q6-beH\",\"articles_thoughts-on-resiliency-and-design_index.md\":\"C1KF8gdv\",\"articles_were-living-in-the-future_index.md\":\"CTH7_1pE\",\"awards_index.md\":\"DmBMhlfs\",\"index.md\":\"0Lw9K3hk\",\"portfolio_amazon-alexa-cortana_index.md\":\"B_yzaVRF\",\"portfolio_amazon-profile_index.md\":\"CpRHlUlT\",\"portfolio_disney-design-system_index.md\":\"BXhEf32r\",\"portfolio_disney-greenlight_index.md\":\"C-15bqqP\",\"portfolio_disney-incubator_index.md\":\"Cf8KAlu3\",\"portfolio_disney-slate_index.md\":\"C47MLefc\",\"portfolio_disney-vr_index.md\":\"Waakn2lL\",\"portfolio_index.md\":\"MxeiDAJa\",\"portfolio_phenomblue-dino-dig_index.md\":\"BVPYnuBp\",\"portfolio_phenomblue-fantastic-future-me_index.md\":\"EyGjPFSV\",\"portfolio_phenomblue-genesis_index.md\":\"DpZtk7r9\",\"portfolio_phenomblue-moppet-mashup_index.md\":\"VrtSz8YN\",\"portfolio_phenomblue-movie-lotto_index.md\":\"CbadzwKI\",\"portfolio_phenomblue-page_index.md\":\"CQXNxqES\",\"portfolio_phenomblue-wild-kingdom_index.md\":\"DkRgeRl3\",\"portfolio_rocket-ironbear_index.md\":\"8a5m9CA2\",\"press_index.md\":\"DsghtLFz\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Jason Bejot\",\"description\":\"The personal website for Jason Bejot\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>